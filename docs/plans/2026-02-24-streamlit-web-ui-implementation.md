# Streamlit Web UI Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a complete Streamlit web application for the shoe image search system with search, batch search, index management, and result comparison features.

**Architecture:** Single Streamlit application with multi-page navigation. Direct integration with existing search_engine.py and vector_index.py. SQLite for search history, file system for uploads.

**Tech Stack:** Streamlit, SQLite, existing CLIP + Faiss backend

---

## Task 1: Setup Project Structure and Configuration

**Files:**
- Create: `utils/__init__.py`
- Create: `utils/history_db.py`
- Create: `config.yaml`
- Create: `data/.gitkeep`
- Create: `uploads/.gitkeep`

**Step 1: Create utils directory and __init__.py**

```bash
mkdir -p utils
touch utils/__init__.py
```

**Step 2: Create data and uploads directories**

```bash
mkdir -p data uploads/temp uploads/history
touch data/.gitkeep uploads/.gitkeep
```

**Step 3: Create config.yaml**

```yaml
app:
  title: "éž‹å­å›¾ç‰‡æœç´¢ç³»ç»Ÿ"
  port: 8501
  max_upload_size: 200

index:
  path: "index"
  auto_load: true

search:
  default_top_k: 10
  default_min_similarity: 0.5
  max_batch_size: 20

storage:
  upload_dir: "uploads"
  history_db: "data/search_history.db"
  temp_file_retention_days: 7
```

**Step 4: Commit**

```bash
git add utils/ config.yaml data/.gitkeep uploads/.gitkeep
git commit -m "feat: add project structure and configuration"
```

---

## Task 2: Create Search History Database Module

**Files:**
- Create: `utils/history_db.py`

**Step 1: Write history_db.py**

```python
"""Search history database management."""
import sqlite3
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional


class SearchHistoryDB:
    """Manages search history in SQLite database."""

    def __init__(self, db_path: str = "data/search_history.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def _init_db(self):
        """Initialize database schema."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS search_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query_image_path TEXT NOT NULL,
                query_image_name TEXT NOT NULL,
                timestamp DATETIME NOT NULL,
                top_k INTEGER NOT NULL,
                min_similarity REAL NOT NULL,
                results_json TEXT NOT NULL
            )
        """)
        conn.commit()
        conn.close()

    def add_search(self, query_image_path: str, query_image_name: str,
                   top_k: int, min_similarity: float, results: List[Dict]) -> int:
        """Add a search record."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO search_history
            (query_image_path, query_image_name, timestamp, top_k, min_similarity, results_json)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (query_image_path, query_image_name, datetime.now(),
              top_k, min_similarity, json.dumps(results)))
        search_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return search_id

    def get_recent_searches(self, limit: int = 100) -> List[Dict]:
        """Get recent search records."""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("""
            SELECT * FROM search_history
            ORDER BY timestamp DESC
            LIMIT ?
        """, (limit,))
        rows = cursor.fetchall()
        conn.close()

        results = []
        for row in rows:
            results.append({
                'id': row['id'],
                'query_image_path': row['query_image_path'],
                'query_image_name': row['query_image_name'],
                'timestamp': row['timestamp'],
                'top_k': row['top_k'],
                'min_similarity': row['min_similarity'],
                'results': json.loads(row['results_json'])
            })
        return results

    def get_search_by_id(self, search_id: int) -> Optional[Dict]:
        """Get a specific search record."""
        searches = self.get_recent_searches(limit=1000)
        for search in searches:
            if search['id'] == search_id:
                return search
        return None

    def cleanup_old_records(self, keep_recent: int = 100):
        """Keep only recent N records."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            DELETE FROM search_history
            WHERE id NOT IN (
                SELECT id FROM search_history
                ORDER BY timestamp DESC
                LIMIT ?
            )
        """, (keep_recent,))
        conn.commit()
        conn.close()
```

**Step 2: Commit**

```bash
git add utils/history_db.py
git commit -m "feat: add search history database module"
```

---

## Task 3: Create Main Streamlit App

**Files:**
- Create: `streamlit_app.py`

**Step 1: Write streamlit_app.py**

```python
"""Main Streamlit application for shoe image search system."""
import streamlit as st
import yaml
from pathlib import Path
from search_engine import ImageSearchEngine
from utils.history_db import SearchHistoryDB
import shutil
from datetime import datetime, timedelta


# Load configuration
@st.cache_resource
def load_config():
    with open('config.yaml', 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# Load search engine
@st.cache_resource
def load_search_engine(index_path: str):
    """Load search engine with caching."""
    try:
        engine = ImageSearchEngine(Path(index_path))
        return engine
    except Exception as e:
        st.error(f"Failed to load search engine: {e}")
        return None


# Initialize database
@st.cache_resource
def get_history_db(db_path: str):
    """Get history database instance."""
    return SearchHistoryDB(db_path)


def cleanup_temp_files(upload_dir: Path, retention_days: int):
    """Clean up old temporary files."""
    temp_dir = upload_dir / "temp"
    if not temp_dir.exists():
        return

    cutoff_time = datetime.now() - timedelta(days=retention_days)
    for file in temp_dir.iterdir():
        if file.is_file() and datetime.fromtimestamp(file.stat().st_mtime) < cutoff_time:
            try:
                file.unlink()
            except Exception:
                pass


def main():
    # Page config
    st.set_page_config(
        page_title="éž‹å­å›¾ç‰‡æœç´¢ç³»ç»Ÿ",
        page_icon="ðŸ‘Ÿ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Load config
    config = load_config()

    # Cleanup temp files on startup
    upload_dir = Path(config['storage']['upload_dir'])
    cleanup_temp_files(upload_dir, config['storage']['temp_file_retention_days'])

    # Load search engine
    index_path = config['index']['path']
    search_engine = load_search_engine(index_path) if config['index']['auto_load'] else None

    # Store in session state
    if 'search_engine' not in st.session_state:
        st.session_state.search_engine = search_engine
    if 'config' not in st.session_state:
        st.session_state.config = config
    if 'history_db' not in st.session_state:
        st.session_state.history_db = get_history_db(config['storage']['history_db'])

    # Main page
    st.title("ðŸ‘Ÿ éž‹å­å›¾ç‰‡æœç´¢ç³»ç»Ÿ")
    st.markdown("---")

    # System status
    col1, col2, col3 = st.columns(3)

    with col1:
        if search_engine:
            stats = search_engine.index.get_stats()
            st.metric("ç´¢å¼•çŠ¶æ€", "âœ… å·²åŠ è½½")
            st.metric("å›¾ç‰‡æ€»æ•°", f"{stats['total_images']:,}")
        else:
            st.metric("ç´¢å¼•çŠ¶æ€", "âŒ æœªåŠ è½½")
            st.warning("è¯·å…ˆåˆ°ç´¢å¼•ç®¡ç†é¡µé¢æž„å»ºæˆ–åŠ è½½ç´¢å¼•")

    with col2:
        history_db = st.session_state.history_db
        recent_searches = history_db.get_recent_searches(limit=10)
        st.metric("æœ€è¿‘æœç´¢", len(recent_searches))

    with col3:
        st.metric("ç³»ç»Ÿç‰ˆæœ¬", "1.0.0")

    st.markdown("---")

    # Quick search
    st.subheader("ðŸ” å¿«é€Ÿæœç´¢")

    if search_engine:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ å›¾ç‰‡è¿›è¡Œæœç´¢",
            type=['png', 'jpg', 'jpeg', 'bmp', 'webp'],
            help="æ”¯æŒæ‹–æ‹½ä¸Šä¼ "
        )

        if uploaded_file:
            col1, col2 = st.columns([1, 2])

            with col1:
                st.image(uploaded_file, caption="æŸ¥è¯¢å›¾ç‰‡", use_container_width=True)

            with col2:
                if st.button("ðŸ” æœç´¢", type="primary"):
                    st.info("è¯·å‰å¾€ 'å›¾ç‰‡æœç´¢' é¡µé¢è¿›è¡Œè¯¦ç»†æœç´¢")
    else:
        st.info("è¯·å…ˆåŠ è½½ç´¢å¼•æ‰èƒ½ä½¿ç”¨æœç´¢åŠŸèƒ½")

    # Navigation guide
    st.markdown("---")
    st.subheader("ðŸ“– åŠŸèƒ½å¯¼èˆª")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.page_link("pages/1_ðŸ”_å›¾ç‰‡æœç´¢.py", label="ðŸ” å›¾ç‰‡æœç´¢", help="ä¸Šä¼ å›¾ç‰‡æœç´¢ç›¸ä¼¼å•†å“")

    with col2:
        st.page_link("pages/2_ðŸ“¦_æ‰¹é‡æœç´¢.py", label="ðŸ“¦ æ‰¹é‡æœç´¢", help="ä¸€æ¬¡æœç´¢å¤šå¼ å›¾ç‰‡")

    with col3:
        st.page_link("pages/3_âš™ï¸_ç´¢å¼•ç®¡ç†.py", label="âš™ï¸ ç´¢å¼•ç®¡ç†", help="ç®¡ç†å›¾ç‰‡ç´¢å¼•")

    with col4:
        st.page_link("pages/4_ðŸ“Š_ç»“æžœå¯¹æ¯”.py", label="ðŸ“Š ç»“æžœå¯¹æ¯”", help="å¯¹æ¯”å¤šä¸ªæœç´¢ç»“æžœ")


if __name__ == "__main__":
    main()
```

**Step 2: Test the main app**

```bash
streamlit run streamlit_app.py
```

Expected: App launches, shows system status (index not loaded warning is OK)

**Step 3: Commit**

```bash
git add streamlit_app.py
git commit -m "feat: add main streamlit app with system status"
```

---

## Task 4: Create Image Search Page

**Files:**
- Create: `pages/1_ðŸ”_å›¾ç‰‡æœç´¢.py`

**Step 1: Write image search page**

